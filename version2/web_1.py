import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory


# åˆå§‹åŒ–å…¨å±€é…ç½®
def init_settings():
    st.set_page_config(
        page_title="GLM-4 Chatbot",
        page_icon="ğŸ¤–",
        layout="centered"
    )
    if "messages" not in st.session_state:
        st.session_state.messages = []


# ä¾§è¾¹æ é…ç½®
def sidebar_config():
    with st.sidebar:
        st.title("ğŸ”§ é…ç½®ä¸­å¿ƒ")
        st.subheader("API è®¾ç½®")
        api_key = st.text_input(
            "API å¯†é’¥",
            type="password",
            value="f037f80bb31908ef8d2159b5e4f17e6d.kDBaHZjpplHWU6pl"
        )
        api_base = st.text_input(
            "API ç«¯ç‚¹",
            value="https://open.bigmodel.cn/api/paas/v4/"
        )

        st.divider()
        st.subheader("æ¨¡å‹å‚æ•°")
        temperature = st.slider(
            "æ¸©åº¦ç³»æ•° (0-2)",
            min_value=0.0,
            max_value=2.0,
            value=0.95,
            step=0.1
        )
        model_name = st.selectbox(
            "æ¨¡å‹é€‰æ‹©",
            ("glm-4-flash", "glm-4", "glm-3-turbo"),
            index=0
        )

    return {
        "api_key": api_key,
        "api_base": api_base,
        "temperature": temperature,
        "model_name": model_name
    }


# åˆå§‹åŒ–å¯¹è¯é“¾
@st.cache_resource
def init_chain(config):
    # 1. åˆå§‹åŒ–å¤§æ¨¡å‹
    llm = ChatOpenAI(
        temperature=config["temperature"],
        model=config["model_name"],
        openai_api_key=config["api_key"],
        openai_api_base=config["api_base"]
    )

    # 2. æ„å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸”å‹å¥½çš„AIåŠ©æ‰‹"),
        MessagesPlaceholder(variable_name="chat_history"),
    ])

    # 3. æ„å»ºå¤„ç†é“¾
    chain = prompt | llm | StrOutputParser()

    # 4. é…ç½®å†å²è®°å½•å­˜å‚¨
    store = {}

    def get_history(session_id: str):
        if session_id not in store:
            store[session_id] = ChatMessageHistory()
        return store[session_id]

    return RunnableWithMessageHistory(
        chain,
        get_history,
        input_messages_key="chat_history",
        history_messages_key="chat_history"
    )


# æµå¼è¾“å‡ºå¤„ç†
def stream_response(prompt, chain):
    with st.chat_message("user"):
        st.markdown(prompt)

    response_container = st.empty()
    full_response = ""

    # æµå¼è°ƒç”¨å¤§æ¨¡å‹
    for chunk in chain.stream(
            {"chat_history": [HumanMessage(content=prompt)]},
            config={"configurable": {"session_id": "default"}}
    ):
        full_response += chunk
        response_container.markdown(full_response + "â–Œ")

    response_container.markdown(full_response)
    return full_response


# ä¸»ç•Œé¢
def main():
    init_settings()
    config = sidebar_config()

    # åˆå§‹åŒ–å¯¹è¯é“¾
    try:
        chain = init_chain(config)
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return

    # å†å²è®°å½•æ˜¾ç¤º
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ç”ŸæˆAIå“åº”
        try:
            ai_response = stream_response(prompt, chain)
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
        except Exception as e:
            st.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}")


if __name__ == "__main__":
    main()